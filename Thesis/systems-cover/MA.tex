\documentclass[11pt,singlecolumn]{scrartcl}
\usepackage{xcolor}
\usepackage[masterthesis]{systems-cover}
\usepackage{amsmath,amssymb,amstext}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage[capposition=bottom]{floatrow}
\usepackage{listings}
\usepackage{url}
\usepackage{titlesec}
\usepackage{minted}
\usepackage{subcaption}
\usepackage{mwe}
\usepackage{amssymb}
\usepackage{ifsym}
\usepackage{amsmath}

\definecolor{bg}{rgb}{0.95,0.95,0.95}
\titleformat*{\section}{\LARGE\bfseries}
\titleformat*{\subsection}{\Large\bfseries}
\titleformat*{\subsubsection}{\large\bfseries}

\lstset{frame=tb,
  language=Java,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny,
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}


\pagestyle{fancy}
\fancyhf{}
\fancyhead[LE,RO]{A High-level Graph Query Language Interface for Differential Dataflow}


\fancyfoot[RE,lO]{Master Thesis}
\fancyfoot[LE,RO]{\thepage}

\newcommand{\rust}[1]{\mintinline{rust}{#1}}
\renewcommand{\headrulewidth}{2pt}
\renewcommand{\footrulewidth}{1pt}

\covernum{164}
\covertitle{A High-level Graph Query Language Interface for Differential Dataflow}
\coverauthor{Lukas Striebel}
\coverdate{October 2016 - April 2017}
\coversupervisedby{Prof.\ Timothy Roscoe\\Dr. Ioannis Liagouris\\Dr. Desislava Dimitrova\\Moritz Hoffmann}


\begin{document}

\hspace{60mm}
\begin{center}
 \textbf{Abstract} \end{center}
In today's business world, Social Networks have become one of the most important if not the most important platforms to advertise and attract new customers. In order to deliver your message to the right people, complicated analysis on the behaviour and structure of these networks has to be done. Thus, graph databases have been increasing in popularity in comparison to traditional relational databases. This tendency has given birth to the need of powerful graph datawarehouses and query evaluators. It is of tremendous importance to capture and extract the key information that lie within these graphs. In this thesis, we present Qlidaf, an Interface of the Property Graph Query Language for Differential Dataflow, capable of fulfilling these tasks.

\clearpage
\tableofcontents
\clearpage
\listoffigures
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
In this chapter, we describe the motivation behind the thesis and which problems we aim to solve. Furthermore we outline the structure of the thesis.
\subsection{Motivation}
In our modern digital age, the Internet is the largest centre of data and knownledge. The websites of large companies are accessed thousands of times a day. Furthermore, the request do not only grow in size, but also become more complex and face higher demands from management. In order to cope with this huge amount of client requests, large sever farms were built. In these farms, hundreds of servers and switches are connected and request have to travel across multiple machines until they are answered. Thus, the need for a system that directs and balances the huge amount of request arises. Additionally, this system should be able to retrieve information from the network and execute algorithms such as Dijkstra's shortest path. The result of these operations are needed to make the right decision when facing with difficult choices in controlling the distribution and processing of the user requests.\\\\
The first step to deal with this challenge is to model this complicated network of computers as a property graph. A property graph is a graph which allows its vertices and edges to posses an unlimited number of attributes. Traditionally, very few attributes are allowed in a graph, e.g. a weight attribute on the edges. However, our porperty graph allows attribute of type string, float and boolean.\\ In a second step, an algorithm determines how the request should best be directed in order to not overload any part of the network. This thesis focuses on the second step and proposes a program that can evaluate queries on a property graph, in order to determine weak links and find the optimal distribution of the work load.\\\\
This thesis combines the new query language PGQL, which is design to aims to unitize graph queries, with a powerful graph analytics framework called Differential Dataflow. This framework provides us with the necessary tools to construct a query plan, execute said plan on a property graph and obtain the desired answer to the user's request.
\subsection{Structure of the Thesis}
The Thesis has the following structure:\\
First, we give an overview of the technologies used in the Thesis in Section 2.
We continue by giving a detailed explanation of the query parser  in Chapter 3. In Section 4, we explain how the program sets up the dataflows according to the query. Next, we describe the experiments that were run in order to determine the performance of the program in Chapter 5. An overview of other graph query languages and query plan generator can be found in section 6. Finally, we draw our conclusion, provide an outlook and show potential for future work.
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Background}
In this section, all the relevant technologies used in the Thesis are explained.
\subsection{The Property Graph Query Language}

The Property Graph Query Language (PGQL) was developed by Oskar van Rest at Oracle\cite{vanRest:2016}. PGQL enables developers to write intuitive path queries over a property graph. The syntax of PGQL greatly resembles the one of SQL. A PGQL query consists of 4 clauses, two of them are optional.
\begin{itemize} 
\item Path Clause (optional)
\item Select Clause (required)
\item Where Clause (required)
\item Solution Modifier Clause (optional)
\end{itemize}
In the following, we will give a short overview of the four clauses. We will go into more detail in section 3, in which we explain precisely how each clause has to be parsed.\\\\
In the \textbf{Path Clause} custom path patterns are defined, which are to be used again in the Where Clause.\\
\textbf{Example:}
\begin{verbatim}
PATH connects_to := (:Generator) --> (:Connector WITH status = 'OPERATIVE')
 \end{verbatim}
\noindent In this example, we define connects\_to as an edge between a generator and an operating connector.\\\\
The \textbf{Select Clause} bears great similarity to the SQL one. Here, the set of properties one wishes to retrieve is defined. Possible Selections are either\\
\begin{itemize} 
\item everything, indicated by the use of * 
\item certain attributes of edges and/or vertices
\item Aggregation of certain attributes of edges and/or vertices.
\end{itemize}
\textbf{Example:}
\begin{verbatim}
SELECT v.name, AVG(v.age)
 \end{verbatim} 
 Currently, there are five types of aggregations supported:\\
 \begin{itemize} 
\item COUNT, returns the number of tuples in the solution 
\item MAX, returns the maximum value of an attribute in any tuple. The specified attribute has to be numeric.
\item MIN, returns the minimum value of an attribute in any tuple. The specified attribute has to be numeric.
\item SUM, returns the sum of an attribute over all the tuples. The specified attribute has to be numeric.
\item AVG, returns the average of an attribute over all the tuple. The specified attribute has to be numeric.
\end{itemize}

The most complex part of the query is the \textbf{Where Clause}. In this section, all the requirements that the edges and vertices of the result set have to fulfill are specified.\\
\textbf{Example:}
\begin{verbatim}
WHERE v.name = 'Alice', v.age > 30, w.name != 'Bob', v -> w
\end{verbatim}

Finally, the result set can be modified in the \textbf{Solution Modifier Clause}. This Clause can be broken down into four subsections, which are all extremly similiar to their SQL counterpart:\\\\
\textbf{Group by} \quad In the group by the developer specifies the attributes that shall be used for the aggregation in the select clause.\\\\
\textbf{Order by} \quad A set of attributes that determine the order of the results, either ascending or descending.\\\\
\textbf{Limit} \quad Provides the maximum cardinality of the result set.\\\\
\textbf{Offset} \quad Denotes how many tuples of the result set shall be skipped.\\\\

\noindent The official PGQL Specifications can be accessed online at \cite{PGQLSpec}

\subsection{Dataflow Programming}
Traditionally, a program is a list of instructions to be executed by the computer. However, there are certain programming paradigms that do not conform to this idea. Dataflow oriented programming is one of them. In the dataflow programming paradigm, a program is modeled as a directed graph of data flowing between operations.  Rather than explicitly defining the order of execution as most other programming paradigms do, dataflow programming specifies a network of nodes, between which the data is exchanged. In program execution, this means until a node's data buffer is non-empty, it remains idle. Only after the node receives an input, it begins to execute the operations that are defined in the program. Once the computation is completed, the resulting data is forwarded to the next node. Each node can be viewed as a black box, since the programmer does not have to worry what happens inside. His responsibilities lie only in the task of connecting the nodes in the correct manner.


\subsection{Differential Dataflow}

Developed by Frank McSherry at Microsoft, Differential dataflow is a data-parallel programming framework designed to efficiently process large volumes of data and to quickly respond to arbitrary changes in input collections.\cite{Differential}\\
Differential Dataflow is an extension of Timely Dataflow, which was first introduced by Murray. \cite{GitTimely}
Rather than rewriting the entire data every time a change occurs, Differential Dataflow only keeps track of the changes made to the data. This allows for huge timesavings when executing the same operation multiple times on different data, since the results of previous computations can be reused.\\\\
\begin{figure}[H]
\includegraphics[width=1\textwidth]{dataflow_example}
\caption[Timely Dataflow Example]{An example of a timely dataflow, Source:\protect\cite{Naiad}}
\end{figure}
Differential Dataflow is open-source and available for download at \cite{GitDiff}.


\subsection{The Rust Programming Language}
Rust is a fairly new programming language. It's origin lies with the Mozilla Company, where former employee Graydon Hoare started development in 2006. In the year 2009, Mozilla offically began sponsering the project. It took another 6 years until the first stable version of the compiler was released. Nowadays most of the development is done by volunteers from the rust community.  The Rust compiler is self-hosted, meaning it is written in Rust as well. Rust won the Award for Most Loved Programming Language in 2016, hosted by the Stack Overflow Community.\\
Rust has been praised as it combines guaranteed memory safety with minimal runtime. Other prominent features are ''zero-cost abstractions, move semantics, threads without data races, trait-based generics, pattern matching, type inference and efficient C bindings''\cite{RustLang}.\\\\
One of Rust's most prominent feature is memory safety. That means it should never be possible to dereference a null pointer or a pointer to an object that has already been deallocated. However, it is still possible to produce out-of-bonds array accesses, since these checks have to be done at runtime.\\
In this thesis, we will often encounter recursive objects, such as trees for example. Since Rust does not have a null type, the children have to be wrapped in an object of type \textit{Box}, in order to avoid infinite recursion.\\\\
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
struct BinaryTree {
	leftChild: Box<BinaryTree>,
	rightChild Box<BinaryTree>,
}
\end{minted} 
\\\\Rust is completly opensource, and a big part of the code was written by members of the community. It is currently licensed under the Apache\footnote{\url{https://www.apache.org/licenses/LICENSE-2.0}} and MIT license\footnote{\url{https://opensource.org/licenses/MIT}}


\subsection{Nom}
Our Query Parser is built using nom, \cite {Nom}. Nom was developed by Geoffroy Couprie. It is a byte oriented, zero copy, streaming Parser Library written in Rust. The library provides macros, functions and enums which faciliate the parsing process. As of today, there exist dozends of parser written with nom, for example for gif, csv and tar files.\\\\
The most commonly used macros were:
\begin{itemize} 
\item do\_parse!: Takes a list of parsers as inputs and applies them sequentially, finally returns a tupel.
\item alt\_complete!: Takes a set of parsers as input and applies them until one succeeds. Returns the result of the first successful parser.
\item opt!: Takes a parsers as input and makes it optional. Returns None if parser was unsuccessful or Some otherwise.
\item tag!: Parses a specific String. Aborts if the String is not found.
\item named!: Faciliates the creation of new custom parsers.
\item many0!: Applies the parser 0 or more times.
\item many1!: Applies the parser 1 or more times. 
\end{itemize}

Nom is open-source and available for download at \cite{GitNom}.

\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Parser}
In the following, we describe how our parser is built. As mentioned in Section 2.1, the Parser has to recognize the following 4 clauses: Path definitions (line 3), select (line 5), where (line 7), solution modifier (line 9). Finally, it returns a Query object (line 10)
\\The main parse function therefore looks as follows:\\
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
named!(pgql_query<Query>,
    do_parse!(
        paths: opt!(path_clause) >>
        space >>
        select: select_clause >>
        space >>
        vvhere: where_clause >>
        space >>
        solmod: opt!(solution_modifier_clause) >>
        (Query { select: select, vvhere: vvhere, paths: paths, solmod: solmod})
     )
);
 \end{minted} 


\subsection{Path Clause}

The Path Clause is a list of definitions. Each Path definition starts with a name (line 16), followed by `:=' (line 18 )and then a path description (line 20). The definition is closed by the begining of a new line (line 6). The paths defined in this clause can then be reused in the Where Clause.

\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
named!(path_clause<Vec<(String, Connection)> >,
    many0!(
        chain!( 
            path: path ~
            opt!(space) ~
            tag!("\n"),
            || path
        )
    )
);

named!(path<(String, Connection)>,
    chain!( 
        tag_no_case_s!("path") ~
        space ~
        name: char_only ~
        opt!(space) ~
        tag!(":=") ~
        opt!(space) ~
        pattern: path_pattern,
        || (String::from(name),pattern)
    )
);

\end{minted}
\subsection{Select Clause}
The Select Clause of PGQL is very similar to the SQL one. Started by the keyword `Select' (line 3), a list of attributes is provided (line 5). If the user wishes to retrieve everything, he is to use the asterisk sign (line 18). Aggregate functions like `Sum', `Avg' etc. may also be accessed (line 12). Attributes may be renamed with the keyword `as', but this function is currently not supported.\\
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
named!(select_clause<Vec<SelectElem> >,
    chain!( 
        tag_no_case_s!("select") ~
        space ~
        select_elems: many1!(select_elem),
        || select_elems
    )
);

named!(select_elem<SelectElem>,
	alt_complete!(
		aggregate => {|aggregation| SelectElem::Aggregation(aggregation)} |
		variable_name => {
			|v| {let (name, field) = v;
			let a = Attribute{name: name, field: field};
			SelectElem::Attribute(a)}
		} |
		tag_s!("*") => {|_| SelectElem::Star}
	)     
);
\end{minted}

\subsection{Where Clause}
The Where Clause consists of a list of Constraints. Each constraint defines either a path or value requirement that has to be fulfilled by the respective vertex or edge.
 
 \subsubsection{Path Constraints}
 Path Constraints are patterns, which are matched against the graph. If a set vertices does not fit the pattern, it is discarded.\\
 A typical Path Constraint requires a vertex to have certain edges to other vertices. For example, the constraint: (v) -> (u) requires that the vertex v has a direct edge to the vertex u.\\\\Vertices are absoultly essential for the path constraints. A Vertex is delimited by brackets () (line 3 \& 29) and its properties are specified inside the brackets (or at a later point in time as a seperate constraint). There are three possible properties, all of them are completly optional:\\
 \begin{itemize}
 \item The name of the vertex. If no name is provided, the vertex is anonymous (line 5).
 \item A set label constraints, recognized by the : sign (line 7). Label constraints differ from value constraints since the labels are defined in a very particular way, unlike the attributes. See section 4.1 for more details. 
 \item A set of inlined value constraints, recognized by the keyword WITH (line 9). This is basically just syntatic sugar, for example, the two formulation (v WITH age \textgreater 10) and (v), v.age \textgreater 10 are completly equal.
 \end{itemize}
 
\textbf{Code sample}\\\\
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
named!(query_vertex<Vertex>,
    delimited!(
        char!('('),
        do_parse!(
            vertex_name: opt!(char_only) >> 
            opt!(space) >>
            label: opt!(label_constraint) >>
            opt!(space) >>
            inlined: opt!(inlined_constraints) >>
            ({                
                let mut constraints: Vec<Expr> = match inlined {
                    Some(value) => value,
                    None => Vec::new()
                };

                match label {
                    Some(value) => {constraints.push(value);},
                    None => {}
                }

                match vertex_name {
                    Some(name) => {Vertex{name: String::from(name),
                                   anonymous: false, constraints: constraints }},
                    None => Vertex{name: String::from(""),
                                   anonymous: true, constraints: constraints }
                }
            })                
        ),
        char!(')')
  )
);
 \end{minted} 
 
 The connection between two vertices can be a very simple or a very sophisticated edge with many attributes. An edge is defined by an arrow sign -\textgreater and its properties are specified inside the brackets. If there are no properties to be defined, the brackets are omitted (line 3 \& 5). There are three parameteres, all of them are completly optional:\\
 \begin{itemize}
 \item The name of the edge. If no name is provided, the edge is anonymous (line 10).
 \item One label constraint, recognized by the : sign (line 12). 
 \item A set of inlined value constraints, recognized by the keyword WITH (line 14). 
 \end{itemize}
 
\textbf{Code sample}\\\\
 \begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
named!(query_edge<Edge>,
        alt_complete!(
            tag!("-->") => { |_| Edge {name: String::from(""),
					 inverted: false, constraints: Vec::new() } } |
            tag!("->") => { |_| Edge {name: String::from(""),
					 inverted: false, constraints: Vec::new() } } | 
            delimited!(
                tag!("-["),
                do_parse!(
                    edge_name: opt!(char_only) >> 
                    opt!(space) >>
                    label: opt!(label_constraint) >>
                    opt!(space) >>
                    inlined: opt!(inlined_constraints) >>
                    ({
                        
                        let mut constraints: Vec<Expr> = match inlined {
                            Some(value) => value,
                            None => Vec::new()
                        };

                        match label {
                            Some(value) => {constraints.push(value);},
                            None => {}
                        }

                        match edge_name {
                            Some(name) => {Edge{name: String::from(name),
					 inverted: false, constraints: constraints }},
                            None => Edge{name: String::from(""), 
					inverted: false, constraints: constraints }
                        }
                    })                
                ),
                tag!("]->")
 );
 \end{minted}
\\\\The inlined and the label constraint are parsed as following:\\\\
 \begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
named!(label_constraint<Expr>,
    chain!(
        char!(':') ~
        opt!(space) ~
        label: char_only, // Label
        || Expr::Label(String::from(label))
    )
);

named!(inlined_constraints<Vec<Expr> >,
    chain!(
        tag_no_case_s!("with") ~
        space ~
        exprs: expressions,
        || exprs
    )
);
 \end{minted}
\\\\The PGQ Language allows to nest an unlimited amount of (vertex,edge,vertex)-tuples in one single constraint. I.e. after parsing an initial vertex, infinitely many vertices proceeded by an edge may follow. To process this chain of tuples, a fold function (line 13) has to be used. While iterating through this vector, we clone the previous vertex, then create an new connection and push it into another vector(line 21). This vector is then returned at the end of the function (line 24).\\\\
 \begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
named!(path_pattern<Vec<Connection> >,
    chain!(
        initial: query_vertex ~ 
        remainder: many1!(
            chain!(
                space ~ 
                edge: query_edge ~
                space ~
                target: query_vertex, 
                || (edge, target)
            )
        ), ||
    fold_connection(initial, remainder) 
    )
);

fn fold_connection(initial: Vertex, remainder: Vec<(Edge, Vertex)>) -> Vec<Connection> {
    let mut result = Vec::new();
    let mut previous = initial;
    for (edge, vertex) in remainder{
        result.push(Connection{source: previous, target: vertex.clone(), edge: edge});
        previous = vertex;
    }
    return result;
}
 \end{minted}
 \subsubsection{Value Constraints}
 Value Constraints are constraints on attributes of the Vertex, e.g. name = `Alice' or age \textless  40. Every Value Constraint has to include one or multiple vertex attributes, and one or more Literals. Literals are raw values, and come in 3 types:
 \begin{itemize} 
\item Strings e.g. `Alice' (line 2)
\item Floats e.g. 40 (line 3)
\item Booleans, which are either true or false (line 4)
\end{itemize}
 
 \begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
pub enum Literal {
    Str(String),
    Float(f32),
    Boolean(bool),
} 
 
named!(literal<Literal>,
    alt_complete!(
        float       => { |f| Literal::Float(f)             } |
        boolean     => { |b| Literal::Boolean(b)           } |
        string      => { |s| Literal::Str(String::from(s)) } 
            
    )
);
 \end{minted}
 \\\\Possible operateros are:
 \begin{itemize}
 \item arithmetric: addition, subtraction, multiplication, division, modulo
 \item comperative: equals, not equals, greater, greater equals, smaller, smaller equals
 \item boolean: and, or, not
 \item string: like. The like is denoted as ~= and tests if a string is contained within another string. For example `lice' ~= `Alice' returns true.
 \end{itemize}
Expressions play a vital part in the parsing of a query.  For simplicity, only the addition and subtraction expression are shown in the following example.\\
 \begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
named!(expression< Expr >, chain!(
    initial: operand ~ 
    remainder: many0!(
            alt!(
                chain!(opt!(space) ~ tag!("+")  ~ opt!(space) ~ op: operand,   || (Oper::Add, op))       |
                chain!(opt!(space) ~ tag!("-")  ~ opt!(space) ~ op: operand,   || (Oper::Sub, op))       
             )
        ), ||
    fold_exprs(initial, remainder) 
    )       
);

fn fold_exprs(initial: Expr, remainder: Vec<(Oper, Expr)>) -> Expr {
    remainder.into_iter().fold(initial, |acc, pair| {
        let (oper, expr) = pair;
        match oper {
            Oper::Add       => Expr::Add(Box::new(acc), Box::new(expr)),
	 Oper::Sub       => Expr::Sub(Box::new(acc), Box::new(expr)),
        }
    })
}
 \end{minted} 
 \subsection{Solution Modifier Clause}
 
 The Solution Modifier Clause consists of three parts: 
  \begin{itemize} 
\item GroupBy Clause
\item OrderClause
\item Offset and Limit Clause
\end{itemize}
 All clauses are optional and can be omitted.\\
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
named!(solution_modifier<(GroupBy, OrderBy, (i32, i32))>,
    chain!(
        group: group_by ~
        space ~
        order: order_by ~
        space ~
        limit: limit_offset,
        || (group, order, limit)
    )
);
 \end{minted}
\\\\ The group by clause parses a number of attributes (line 7), which are returned as a vector. Every attribute can be either ascending or descending(line 22 \& 23).\\\\
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
named!(group_by<GroupBy>,
    chain!(
        tag_no_case_s!("group by") ~
        space ~
        variable_names: many1!(
            chain!(
                variable_name: attribute ~ 
                opt!(space) ~
                opt!(char!(',')),
                || variable_name
            )
        ),
        || GroupBy{ group: variable_names}
    )
);
\end{minted}
\\\\ The order by clause parses a number of variable fields, which are returned as a vector. Every attribute can be either ascending or descending.\\\\
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
named!(order_by<OrderBy >,
    chain!(
        tag_no_case_s!("order by") ~
        space ~
        order_terms: many1!(
            chain!(
                order_term: order_term ~
                opt!(space) ~
                opt!(char!(',')),
                || order_term
                )
            ),
        || OrderBy{order: order_terms}
    )
);

named!(order_term<OrderTerm>,
        chain!(
            variable_name: attribute ~
            space ~
            asc: alt!(
                    tag_no_case_s!("asc") => { |_| true } |
                    tag_no_case_s!("desc") => { |_| false } |
                    tag!("") => { |_| true }
                ),
            || {OrderTerm {variable_name: variable_name, asc: asc}}
        )
);
\end{minted}
\\\\ The limit and offset clause parses two integers (line 5 \& 9), which are returned as a pair (line 10).\\\\
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
named!(limit_offset<(i32, i32)>,
    chain!(
        tag_no_case_s!("limit") ~
        space ~
        limit: unsigned_int ~
        space ~
        tag_no_case_s!("offset") ~
        space ~
        offset: unsigned_int,
        || (limit, offset)
    )
);
\end{minted}
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Query Evaluation}
In this chapter we will discuss how we construct the dataflow from our queries.
\begin{figure}[H]
\includegraphics[width=1\textwidth]{pgr}
\caption[Query Evaluation Overview]{An overview of query evaluation}
\end{figure}
\subsection{Graph Loader}
The loading of the graph plays a major part of the entire program execution time. To load a graph from a text file into differential Dataflow, an entirely new parser had to be written. This parser was also built using nom. In order fo it to properly recognize a graph, the graph has to be supplied in the Vertex-Edge-List format\cite{EdgeList}. This format gives us a text file (.txt), which contains two lists. First, all the vertices and their attributes are described. The vertices are ordered ascing according to their ID. The IDs have to start at 1 and must be continuous.\\\\
\textbf{Small sample graph}
\begin{lstlisting}
1 * { 'Server' }             name:'center'   ip:'192.168.0.0'   ram:4
2 * { 'VM' }                 name:'node1'    ip:'192.168.0.1'   ram:8
3 * { 'Firewall' 'VM' }      name:'node2'    ip:'192.168.0.2'   ram:16.5
4 * { 'Host' 'Server' 'VM' } name:'node3'    ip:'192.168.0.3'   ram:32.5
5 * { 'Switch' }             name:'node4'    ip:'192.168.0.4'   ram:64.5
1 2 'connects' bandwidth:1.5 utilization:0.14
2 3 'connects' bandwidth:2.1 utilization:0.22
2 4 'connects' bandwidth:1.1 utilization:0.38
4 5 'connects' bandwidth:1.3 utilization:0.58
\end{lstlisting}

\noindent A complete graph consists of at least one node (line 3) and at least one edge (line 4). Therefore, the graph parser looks as folllows:\\\\
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
named!(graph_parser<Graph>,
    chain!(
        nodes: many1!(node) ~
        edges: many1!(edge),
        || Graph {nodes: nodes, edges: edges}
    )
);
\end{minted}
\\\\Each vertex begins with an integer (line 3), representing the ID of the vertex, follow by an asterisk (line 5). Subsequently the lables of the vertex are presented (line 7), delimited by curly brackets. At least one label has to be provided whereas an upper limit does not exist. Finally, all the attributes of the vertex are presented in a JSON-like fashion (line 9). \\
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
named!(node<Node>,
    chain!(
        id: unsigned_int ~
        space ~
        char!('*') ~
        space ~
        labels: labels ~
        space ~
        attr: attributes ~
        line_end,
        || {
            let mut map = HashMap::with_capacity(attr.len());
            for elem in attr{
                let (name, value) = elem;
                map.insert(String::from(name), value);
            }
            Node {id: id, label: labels, attribute_values: map }            
        }
    )

);
\end{minted}
\\\\The label constraints are just a vector of strings (line 7), delimited by curly brackets (line 3 \& 11).\\\\
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
named!(labels<Vec <String> >, 
    chain!(
        char!('{') ~
        opt!(space) ~
        labels: many1!(
            chain!(
                s: string ~
                opt!(space),
                || String::from(s)
            ) ~ 
        char!('}'),
        || labels
    )
);
\end{minted}
\\\\Each attribute begins with a name (line 4), followed by a : (line 6) and terminated by the actual value (line 8). \\\\
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
named!(attributes< Vec< (&str, Literal) > >,
    many0!(
        chain!(
            name: char_only ~
            opt!(space) ~
            char!(':') ~
            opt!(space) ~
            value: literal ~
            opt!(space),
            || (name, value)
        )
    )
);
\end{minted}
\\\\The edges of the graph are parsed in a very similiar way. There are two main differences: instad of an asterix, a second integer is given denoting the ID of the target vertex (line 5), while the first integer determines the ID of the source vertex. Additionally, edges must have exactly one single label. The rest remains unchanged in comparison to the vertex parser.\\\\
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
named!(edge<GraphEdge>,
    chain!(
        source: unsigned_int ~
        space ~
        target: unsigned_int ~
        space ~
        label: string ~
        space ~
        attr: attributes ~
        line_end,
        || {
            let mut map = HashMap::with_capacity(attr.len());
            for elem in attr{
                let (name, value) = elem;
                map.insert(String::from(name), value);
            }
            GraphEdge {source: source, target: target,
		label:String::from(label), attribute_values: map }
        }
    )
);
\end{minted}

\subsection{Construction of the Dataflow}
The biggest challenge in the thesis was translating the PGQL query into a dataflow. As described in chapter 3, a query contains a vector of projections and a vector of constraints. We chose to apply the projections at the vey latest point in time. Thus, our evaluation begins with the second vector.
\subsection{Constraint sorting}
First, we iterate through the vector and sort the constraints by type:\\
\begin{itemize}
\item If the constraint is a path pattern, it is put into a seperate vector (line 5).
\item If the constraint is an expression, we determine the name of the involved vertex or edge by recursively exploring the expression (line 7). Once the variable name is known, the expression is then pushed into a Hashmap of Vectors (line 10 \& 14). For each variable there exists a seperate vector with expressions. This grouping helps tremendously when filtering the vertex set later on.
\end{itemize}
\textbf{Code sample}\\\\
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
let mut connections = Vec::new();
let mut selections : HashMap<String, Vec<Expr> > = HashMap::new();
for constraint in &vvhere{
                match constraint {
                    &Constraint::PathPattern(ref pattern) => connections.push(pattern),
                    &Constraint::Expr(ref expr) => {
                        let name = explore_expr((*expr).clone());
                        let mut new = false;
                        match selections.get_mut(&name) {
                            Some(vec) => vec.push((*expr).clone()),
                            None => new = true,
                        }
                        if new {
                            selections.insert(name, vec![(*expr).clone()]);
                        }
                    },
                }
            }
\end{minted}
\subsection{Constructing an execution plan}
Once we have iterated over the entire vector, we begin the construction of an execution plan. We iterate through all the path patterns (we have stored them previously in a seperated vector) and make a decision how to construct the plan. There are three possibilities:
\begin{itemize}
\item Both the source and the target of the edge have not appeared before. In this case, we pull the source and the target set and join them with the edge set. 
\item The source of the edge has already appeared before. In this case, we pull the target set, map the result we carried onto the correct id and then join them with the edge set.
\item Both the source and the target of the edge have appeared before. In this case, we are dealing with a loop in the query. Thus, we do not need to join any sets, but we can just enforce an additional constraint on the ids.
\end{itemize}
\textbf{Code sample}\\\\
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
for connection in connections {
                if !used_fields.contains(&connection.source.name) {
                    used_fields.insert(&connection.source.name);
                    used_fields.insert(&connection.target.name);
                    names.insert(&connection.source.name, ids);
                    ids = ids + 1;
                    names.insert(&connection.target.name, ids);
                    ids = ids + 1;

                    let mut new = false;
                    match selections.get_mut(&connection.source.name) {
                            Some(mut vec) => vec.append(& mut connection.source.constraints.clone()),
                            None => new = true,
                    }
                    if new {
                            selections.insert(connection.source.name.clone(), connection.source.constraints.clone());
                    }
                    new = false;
                    match selections.get_mut(&connection.target.name) {
                            Some(mut vec) => vec.append(& mut connection.target.constraints.clone()),
                            None => new = true,
                    }
                    if new {
                            selections.insert(connection.target.name.clone(), connection.target.constraints.clone());
                    }

                    execution_plan.push(
                        PhyPlan{ 
                            name: vec![connection.source.name.clone(), connection.target.name.clone()],
                            left: connection.source.name.clone(),
                            right: connection.target.name.clone(),
                            join_id: 0,
                            join: true,
                            filter_id: 100,
                            constraints: connection.edge.constraints.clone(),
                        }
                    );
                }
\end{minted}
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
                else if !used_fields.contains(&connection.target.name) {
                    used_fields.insert(&connection.target.name);
                    names.insert(&connection.target.name, ids);
                    ids = ids + 1;

                    let mut new = false;
                    match selections.get_mut(&connection.target.name) {
                            Some(mut vec) => vec.append(& mut connection.target.constraints.clone()),
                            None => new = true,
                    }
                    if new {
                            selections.insert(connection.target.name.clone(), connection.target.constraints.clone());
                    }
                    execution_plan.push(
                        PhyPlan{ 
                            name: vec![connection.source.name.clone(), connection.target.name.clone()],
                            left: connection.source.name.clone(),
                            right: connection.target.name.clone(),
                            join_id: *names.get(&connection.source.name).unwrap(),
                            join: true,
                            filter_id: 100,
                            constraints: connection.edge.constraints.clone(),
                        }
                    );
                }
                else {
                    execution_plan.push(
                        PhyPlan{ 
                            name: vec![connection.source.name.clone(), connection.target.name.clone()],
                            left: connection.source.name.clone(),
                            right: connection.target.name.clone(),
                            join_id: *names.get(&connection.source.name).unwrap(),
                            join: false,
                            filter_id: *names.get(&connection.target.name).unwrap(),
                            constraints: connection.edge.constraints.clone(),
                        }
                    );
                }

            }
\end{minted}
\subsection{Selection on the vertex set}
The next step is to apply all the selections we have sorted beforehand on the vertex set. For each non-anonymous vertex, we filter the entire set (line 3) and save the result in a Hashmap (line 6). The key in this Hashmap is the name of the vertex.\\
\textbf{Code sample}\\\\
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
let mut result = HashMap::new();
for (name, filter) in selections {
    let result = vertices.filter(move |x| {
        check_node(&x, &filter)
    });
    results.insert(name, result);
}
\end{minted}
\subsection{Executing the plan}
After constructing a plan, we iterate over through all the steps we assemble in the chapter 4.4 (line 2). In the beginng, our result, which is a Collection of Vectors of Vertices, is empty (line 1). We begin by pulling the source's vertex collection from the Hasmap we defined in section 4.5 (line 4 \& 8). The Hashmap's key is the name of the vertex. Once we have the correct vertex collections, we join it with the edge set (line 13), and map the result (line 15), in order to keep track of the vertex ID of the target. We then repeat this step for the target vertex. We end up with a vector of vertices, which we store as the result.\\\\
If there are more elements in the execution plan, we repeat the previous steps until we are done. The only difference is that we reuse our previous result, instead of pulling a new collection from our vertex Hashmap.\\
\textbf{Code sample}\\\\
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
let mut result = None;
for step in execution_plan {   
    if step.join && step.join_id == 0 {
        let sources = match plans.get(&step.left){
            None => vertices,
            Some(list) => list,
        };
        let targets = match plans.get(&step.right){
            Some(list) => list,
            None => vertices,
        };
        result =    Some(sources.map(|x| (x.id, x))
                    .join(&edges.filter(move |x| check_edge(&x, &step.constraints))
                    .map(|x| (x.source,x.target)))
                    .map(|(_,v1,v2)| (v2,v1))
                    .join(&targets.map(|x| (x.id, x)))
                    .map(|(_,v1,v2)| vec![v1,v2]));
    }
    else if !step.join {
        let int = step.join_id;
        let int2 = step.filter_id;
        result = Some(result.unwrap().map(move |x| (x[int].id, x))
                .join(&edges.filter(move |x| check_edge(&x, &step.constraints))
                .map(|x| (x.source,x.target)))
                .filter(move |x| {let &(ref key, ref vec, ref id) = x; vec[int2].id == *id})
                .map(|(_,v1,_)| v1));
    }
    else {                 
        let targets = match plans.get(&step.right){
            Some(list) => list,
            None => vertices,
        };
        result = Some(result.unwrap().map(move |vec| (vec[step.join_id].id, vec))
                .join(&edges.filter(move |x| check_edge(&x, &step.constraints))
                .map(|x| (x.source,x.target)))
                .map(|(_,v1,v2)| (v2,v1))
                .join(&targets.map(|x| (x.id, x)))
                .map(|(_, mut v1,v2)| {v1.push(v2);v1}));   
    }
}
 \end{minted} 
\subsection{Applying the projection and aggregation}
The final step is to apply the projections and the aggregation that were passed in a vector. In order to do get the desired result, we iterate over this vector (line 1) and take the appropriate action depending what attribute is asked for by the user. In case of an aggregation (line 11), we first group the records (line 14), and then count, sum  etc. the values (line 15 -- 19).\\
\textbf{Code sample}\\\\
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
for projection in &select{
    match projection {
        &SelectElem::Star => {
            list.inspect(|&(ref x,_)| println!("{:?}", x));
        },
        &SelectElem::Attribute(ref attr) => {
            let field = string_to_static_str(attr.field.clone());
            let id = *(names.get(&attr.name).unwrap());
            list.inspect(move |&(ref x,_)| println!("{:?}", x[id].get(&field.into()).unwrap()));
        },
        &SelectElem::Aggregation(ref aggr) => {
            match aggr {
                &Aggregation::Count(ref attr) => {
                        list.map(move |x| (1, x)).group(move |key, vals, output| {
                                let mut count = 0;
                                for _ in vals {
                                    count = count + 1;
                                }                                                        
                                output.push(((),count));
                            }).inspect(|&(_, ref x)| println!("{:?}", x));        
                }
            }
        }
    }
}
 \end{minted}
\subsection{Evaluating an expression}
It is of great importance to evaluate the expressions formulated in the query in the dataflow. Since expression can be nested an umlimited number of times, the only way to evaluate them is by using a recursive function. This function first evaluates the right hand side, then the left hand side and finally applies the desired operator (line 3). Exceptions are the Not Operater (line 12), which has only one side and expressions which can be evaluated immediately. Under this category falls the Literal (line 19), the Attribute of a vertex (line 20), the built-in functions (line 27) and the label operator (line 13), which simply checks whether the requested label is present in the vertex' label vector.\\\\
\begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg,
linenos]
{rust}
fn evaluate_expr (constraint: Expr, node: &Node) -> Literal {
    match constraint{
        Expr::Equal(left, right)         => Literal::Boolean(evaluate_expr(*left, node) == evaluate_expr(*right, node)),
        Expr::NotEqual(left, right)      => Literal::Boolean(evaluate_expr(*left, node) != evaluate_expr(*right, node)),
        Expr::Smaller(left, right)       => evaluate_expr(*left, node).smaller(evaluate_expr(*right, node)),
        Expr::SmallerEq(left, right)     => evaluate_expr(*left, node).smaller_eq(evaluate_expr(*right, node)),
        Expr::Greater(left, right)       => evaluate_expr(*left, node).greater(evaluate_expr(*right, node)),
        Expr::GreaterEq(left, right)     => evaluate_expr(*left, node).greater_eq(evaluate_expr(*right, node)),
        Expr::Like(left, right)          => evaluate_expr(*left, node).contains(evaluate_expr(*right, node)),
        Expr::And(left, right)           => evaluate_expr(*left, node).and(evaluate_expr(*right, node)),
        Expr::Or(left, right)            => evaluate_expr(*left, node).or(evaluate_expr(*right, node)),
        Expr::Not(value)                 => evaluate_expr(*value, node).not(),
        Expr::Label(label)               => Literal::Boolean(node.label.contains(&label)),
        Expr::Add(left, right)           => evaluate_expr(*left, node).add(evaluate_expr(*right, node)),
        Expr::Sub(left, right)           => evaluate_expr(*left, node).sub(evaluate_expr(*right, node)),
        Expr::Mul(left, right)           => evaluate_expr(*left, node).mul(evaluate_expr(*right, node)),
        Expr::Div(left, right)           => evaluate_expr(*left, node).div(evaluate_expr(*right, node)),
        Expr::Modulo(left, right)        => evaluate_expr(*left, node).modulo(evaluate_expr(*right, node)),
        Expr::Literal(value)             => value,
        Expr::Attribute(attribute)       => {
            match node.attribute_values.get(&attribute.field) {
                Some(literal) => (*literal).clone(),
                None => Literal::Boolean(false),
                //panic!("Field {:?} does not exist!", &attribute.field) 
                }
            },
        Expr::BuiltIn(_, function) => {
            match function {
                BuiltIn::Label => Literal::Str(node.label[0].clone()),
                BuiltIn::Id => Literal::Float(node.id as f32),
                _ => panic!("Function {:?} not supported for nodes", function)
            }
        }
    }
}

 \end{minted}
 \subsection{Execution}
 Since our program is a modul for differential dataflow, the differential dataflow repositiory has to be present in order to execute the program.\\\\
\textbf{Execution Example}\quad The program can be run using the following command:\\\\
 \begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg]
{bash}
$ cargo run --release --example pgql graph.txt query.txt true -- -w 32
\end{minted}
\\The --release argument allows for compiler optimizations and should speed up the execution of the program. The --example pgql argument specifies which program to run. The remaining 4 arguments are parsed by our program. First, we denote the name of the graph we want to parse. This argument has to be a text file with the graph in edge-list format (see section 4.1). The second argument gives a text file which contains all the queries that the user wishes to evaluate. Next a boolean specifies whether the edges in the graph are directed or not. In case they are not, all the edges in the graph text file will be cloned, inverted and added to the edge set. This means an edge from A to B posses the exact same properties as an edge from B to A. If this is not desired, then this argument should be set to false. Finally, we provide a number of arguments needed by the timely framework\footnote{\url{http://www.frankmcsherry.org/timely-dataflow/timely/execute/fn.execute\_from\_args.html}}. First, we define how many worker threads should be used to evaluate the query with the -w argument. This number is expected to be less or equal to the number of cores of the machine. Alternatively, one can add a -n argument to specify the number of processes an a -p argument to denote the ID of this process. If the program is to be distrbute over multiple machines, the -h argument should specify a text file in which all the hostnames and ports are defined.\\\\
\textbf{Benchmark Example}
We used a special python script with the goal to automate the benchmarking of our program.\\\\
 \begin{minted}[
frame=lines,
framesep=2mm,
baselinestretch=1.2,
fontsize=\footnotesize,
bgcolor=bg]
{bash}
$ python run_benchmark.py graph.txt query.txt
\end{minted}
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results and Discussion}

\subsection{Hardware}
All the experiments were conducted on an ETH server (exact model?). This machine possesses 4 x 12 AMD Opteron 6174 processors, each one running at 2.2 GHz. The operating System was Debian 7.0 and the installed memory was 128 GB of RAM.
\subsection{Topologies}
We select two distinctly different topologies: fattree~\cite{FatTree} and jellyfish~\cite{Jellyfish}. Fattree is a modification of the leaf-spine structure, commonly found in datacenters, and it is thus a practically deployable solution.
\begin{figure}[H]
\includegraphics[width=1\textwidth]{fattree}
\caption[Fat-Tree Topology Example]{Example 4-ary Fat-Tree Topology: 16 hosts and 20 4-port switches. Source: \protect\cite{FatTree}}
\end{figure}
Jellyfish is inspired by work on random graphs showcasing that random structures can deliver shorter average paths but is more of an ideal-case reference rather than a realistic topology.
\begin{figure}[H]
\includegraphics[width=0.5\textwidth]{jelly}
\caption[Jellyfish Topology Example]{Jellyfish topology with 16 hosts and 20 4-port switches. Source: \protect\cite{Jellyfish}}
\end{figure}
In the experiment, four different sized topologies were used:\\\\
\textbf{Large Fattree} \quad  The large fattree topology is the largest one of the four topologies. It contains 55'000 vertices, of which 2'880 are switches. Furthermore the graph posses a total of 50'000 bidirectional edges. Since one bidirectional edge is represented by two directed edges, there are a total of 100'000 edges in the collection.\\\\
\textbf{Small Fattree} \quad The small fattree topology has the same vertices to edge ratio as the large fattree, but it is overall much smaller. It contains 17'665 vertices, of which 1'280 are switches. Furthermore the graph possesses a total of 16' 400 bidirectional edges. (needs better formulation) \\\\
\textbf{Large Jellyfish} \quad  The large jellyfish topology matches the large fattree in terms of vertices, but contains far less edges. It contains 56'160 vertices, of which 864 are switches. Furthermore the graph possesses a total of 6'910 bidirectional edges. (needs better formulation)\\\\
\textbf{Small Jellyfish} \quad The small jellyfish topology is the smallest one of the four topologies. It contains 16'770 vertices, of which 390 are switches. Furthermore the graph possesses a total of 2'145 bidirectional edges. (needs better formulation)
\subsection{Expectations}
In all topologies, weight is an attribute of the edges and uniformly distributed from 1 to 9. Consequently, the selection 'weight \textless 2' will therefore remove around 89\% of all edges. A lesser amount of edges immensely shortens the duration of any subsequent join process.\\ Since joining the entire vertex and edge set is a very costly operation, it is possible that queries with multiple, but restricted joins are faster than such, which contain fewer joins, but do not include any restrictions on the edge set. \\\\
Furthermore, it is important to consider the fact that the attributes of vertices and edges in a Differential Dataflow Collection are stored in a Vector of (String, Literal) tuples. We chose this suboptimal implementation since Differential Dataflow does not aprreciate HashMaps at all. However, in order to do a selection on either set, this Vector has to be transformed back into a HashMap for each edge respectively vertex. This process can be quite timeconsuming, therefore the more selections a query contains, the slower it expected to be.\\\\
It should also be noted that one single edge in the query triggers two joins in the evaluation. Reason being that we have to join the edge collection twice with the vertex collection, in order to determine all adjacent vertices of a given vertex.\\\\\\
In total, we ran seven different queries, with each subsequent query being more complex than the preceding one. For each query the resulting dataflow is drawn on the following pages. The numbers on the edges indicate the size of (intermediate) result set on the large fattree.\clearpage
\subsection{Overview}
\begin{figure}[H]
\includegraphics[width=1\textwidth]{ov}
\caption[Query Results Overview]{An overview of the seven queries, run with 32 workers}
\end{figure}
Throughout all seven queries, the following pattern is noticeable:\\
The influence of the number of workers behaves unexpectedly, especially for queries 1,4 and 6. In a perfect world, the query latency would be inversely proportional to the number of workers. However, we only observe this behaviour for query 5, and only for the fattree topologies.\\More often than not, an increase of worker threads leads to a higher latency, especially when the latency is already quite low, around the 50 to 200 millisecond range. We reason that this happens because at that level, the communication overhead dominates the time savings achieved by parallelism. This claim is additionally supported by the fact, that query 5 has one of the longest latencies, ranging up to 10 seconds. When evaluating queries with lower latency, the contention between the workers increases and we frequently observe that the best performance is achieved when running the query with just a single worker.\\\\
Furthermore graphs with smaller sizes show larger variance than their counterpart when run with more than four workers. This behaviour is due to the fact that the time spent coordination the different threads plays a bigger role in small graphs. The time savings achieved by parallelization are outweighed by the communication overhead.\\\\
The following pages contain the dataflows executing the query and the evaluation times distribution for each of the seven queries. While describing the dataflows, we use the following notation:\\\\
\textbf{Projections} are denoted by the symbol \(\pi\). They always occur at the very end of a query.\\\\
\textbf{Selections} are denoted by the symbol \(\sigma\)..\\\\
\textbf{Selections} are denoted by the symbol \(\bowtie\).  .\\\\

\clearpage

\subsection{Query 1}
\begin{verbatim}
SELECT u.name WHERE u.label() = 'switch', u.position = 'access'
\end{verbatim}
\textbf{Peak Memory: 377'840 kb}\\
\textbf{Result size: 1'152} \\
2 simple selections, 0 joins. One pass through the vertices collection is enough to produce the result.\\\\
\textbf{Dataflow}
\begin{figure}[H]
\includegraphics[width=0.2\textwidth]{graph1}
\caption{Dataflow Query1}
\end{figure}
\clearpage
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=\textwidth]{boxesfl/q1}
        \caption[Network2]%
        {{\small Large Fat Tree}}    
        \label{fig:mean and std of net14}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}  
        \centering 
        \includegraphics[width=\textwidth]{boxesfs/q1}
        \caption[]%
        {{\small Small Fat Tree}}    
        \label{fig:mean and std of net24}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.475\textwidth}   
        \centering 
        \includegraphics[width=\textwidth]{boxesjl/q1}
        \caption[]%
        {{\small Large Jellyfish}}    
        \label{fig:mean and std of net34}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.475\textwidth}   
        \centering 
        \includegraphics[width=\textwidth]{boxesjs/q1}
        \caption[]%
        {{\small Small Jellyfish}}    
        \label{fig:mean and std of net44}
    \end{subfigure}
    \caption[  Boxplots for query 1 ]
    {\small Boxplots for query 1} 
    \label{fig:mean and std of nets}
\end{figure*}
\clearpage
\subsection{Query2}
\begin{verbatim}
SELECT n.name WHERE (n) -[e with weight < 4]-> (m)\end{verbatim}

\textbf{Peak Memory: 3'597'452 kb}\\
\textbf{Result size: 36'542}\\
2 Joins and 1 Selection. The joins are not very expensive since we only use about 30\% of the edges.\\\\
\textbf{Dataflow}
\begin{figure}[H]
\includegraphics[width=0.4\textwidth]{graph2}
\caption{Dataflow Query2}
\end{figure}
\clearpage
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=\textwidth]{boxesfl/q2}
        \caption[Network2]%
        {{\small Large Fat Tree}}    
        \label{fig:mean and std of net14}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}  
        \centering 
        \includegraphics[width=\textwidth]{boxesfs/q2}
        \caption[]%
        {{\small Small Fat Tree}}    
        \label{fig:mean and std of net24}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.475\textwidth}   
        \centering 
        \includegraphics[width=\textwidth]{boxesjl/q2}
        \caption[]%
        {{\small Large Jellyfish}}    
        \label{fig:mean and std of net34}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.475\textwidth}   
        \centering 
        \includegraphics[width=\textwidth]{boxesjs/q2}
        \caption[]%
        {{\small Small Jellyfish}}    
        \label{fig:mean and std of net44}
    \end{subfigure}
    \caption[  Boxplots for query 2 ]
    {\small Boxplots for query 2} 
    \label{fig:mean and std of nets}
\end{figure*}
\clearpage
\subsection{Query3}
\begin{verbatim}
SELECT n.name WHERE (n:switch) -> (m with position = 'distribution')\end{verbatim}

\textbf{Peak Memory: 1'433'376 kb}\\
\textbf{Result size: 55'296}\\
2 Joins and 2 Selections. I expected this query to be a little bit slower since we are using all the edges in the joins, but it is only a tiny bit slower than Query 2.\\\\
\textbf{Dataflow}
\begin{figure}[H]
\includegraphics[width=0.6\textwidth]{graph3}
\caption{Dataflow Query3}
\end{figure}
\clearpage
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=\textwidth]{boxesfl/q3}
        \caption[Network2]%
        {{\small Large Fat Tree}}    
        \label{fig:mean and std of net14}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}  
        \centering 
        \includegraphics[width=\textwidth]{boxesfs/q3}
        \caption[]%
        {{\small Small Fat Tree}}    
        \label{fig:mean and std of net24}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.475\textwidth}   
        \centering 
        \includegraphics[width=\textwidth]{boxesjl/q3}
        \caption[]%
        {{\small Large Jellyfish}}    
        \label{fig:mean and std of net34}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.475\textwidth}   
        \centering 
        \includegraphics[width=\textwidth]{boxesjs/q3}
        \caption[]%
        {{\small Small Jellyfish}}    
        \label{fig:mean and std of net44}
    \end{subfigure}
    \caption[  Boxplots for query 3 ]
    {\small Boxplots for query 3} 
    \label{fig:mean and std of nets}
\end{figure*}
\clearpage
\subsection{Query4}
\begin{verbatim}
SELECT n.name WHERE (n with position = 'distribution')
-[e with weight > 8]-> (m with position = 'access')\end{verbatim}

\textbf{Peak Memory: 1'114'896 kb}\\
\textbf{Result size: 3'097}\\
2 Joins and 3 Selections. Even though this query has more constraints than Query 2 and 3, it is faster since we only use 10\% of the edges in the joins. \\\\
\textbf{Dataflow}
\begin{figure}[H]
\includegraphics[width=0.6\textwidth]{graph4}
\caption{Dataflow Query4}
\end{figure}
\clearpage
\begin{figure*}
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{boxesfl/q4}
            \caption[Network2]%
            {{\small Large Fat Tree}}    
            \label{fig:mean and std of net14}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{boxesfs/q4}
            \caption[]%
            {{\small Small Fat Tree}}    
            \label{fig:mean and std of net24}
        \end{subfigure}
        \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{boxesjl/q4}
            \caption[]%
            {{\small Large Jellyfish}}    
            \label{fig:mean and std of net34}
        \end{subfigure}
        \quad
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{boxesjs/q4}
            \caption[]%
            {{\small Small Jellyfish}}    
            \label{fig:mean and std of net44}
        \end{subfigure}
        \caption[  Boxplots for query 4 ]
        {\small Boxplots for query 4} 
        \label{fig:mean and std of nets}
    \end{figure*}
\clearpage

\subsection{Query5}
\begin{verbatim}
SELECT v.name WHERE (u WITH position = 'access')
 -> (v WITH position = 'distribution')
 -> (w WITH position = 'core')\end{verbatim}
\textbf{Peak Memory: 3'797'976 kb}\\
\textbf{Result size: 663'552}\\
4 Joins and 3 Selections. The last 3 queries all contain multiple query edges and are much slower. Since there is no selection on the edge, this particular query is quite slow even though it has "only" 4 joins.\\\\
\textbf{Dataflow}
\begin{figure}[H]
\includegraphics[width=0.6\textwidth]{graph5}
\caption{Dataflow Query5}
\end{figure}
\clearpage
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=\textwidth]{boxesfl/q5}
        \caption[Network2]%
        {{\small Large Fat Tree}}    
        \label{fig:mean and std of net14}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}  
        \centering 
        \includegraphics[width=\textwidth]{boxesfs/q5}
        \caption[]%
        {{\small Small Fat Tree}}    
        \label{fig:mean and std of net24}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.475\textwidth}   
        \centering 
        \includegraphics[width=\textwidth]{boxesjl/q5}
        \caption[]%
        {{\small Large Jellyfish}}    
        \label{fig:mean and std of net34}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.475\textwidth}   
        \centering 
        \includegraphics[width=\textwidth]{boxesjs/q5}
        \caption[]%
        {{\small Small Jellyfish}}    
        \label{fig:mean and std of net44}
    \end{subfigure}
    \caption[  Boxplots for query 5 ]
    {\small Boxplots for query 5} 
    \label{fig:mean and std of nets}
\end{figure*}
\clearpage

\subsection{Query6}
\begin{verbatim}
SELECT v.name WHERE (u WITH position = 'access')
 -[e with weight  < 2]-> (v WITH position = 'distribution')
 -[f with weight > 8]-> (w WITH position = 'core')\end{verbatim}
\textbf{Peak Memory: 1'573'940 kb}\\
\textbf{Result size: 8'334}\\
4 Joins and 5 Selections. Like query 4, the evaluation time goes down as we add more constraints on the edges. Since we have a lot less tuples in the 3rd and 4th join, this query is faster than previous one.\\\\
\textbf{Dataflow}
\begin{figure}[H]
\includegraphics[width=0.8\textwidth]{graph6}
\caption{Dataflow Query6}
\end{figure}
\clearpage
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=\textwidth]{boxesfl/q6}
        \caption[Network2]%
        {{\small Large Fat Tree}}    
        \label{fig:mean and std of net14}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}  
        \centering 
        \includegraphics[width=\textwidth]{boxesfs/q6}
        \caption[]%
        {{\small Small Fat Tree}}    
        \label{fig:mean and std of net24}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.475\textwidth}   
        \centering 
        \includegraphics[width=\textwidth]{boxesjl/q6}
        \caption[]%
        {{\small Large Jellyfish}}    
        \label{fig:mean and std of net34}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.475\textwidth}   
        \centering 
        \includegraphics[width=\textwidth]{boxesjs/q6}
        \caption[]%
        {{\small Small Jellyfish}}    
        \label{fig:mean and std of net44}
    \end{subfigure}
    \caption[  Boxplots for query 6 ]
    {\small Boxplots for query 6} 
    \label{fig:mean and std of nets}
\end{figure*}
\clearpage
\subsection{Query7}
\begin{verbatim}
SELECT v.name WHERE (u WITH position = 'access')
 -[e with weight  < 2]-> (v WITH position = 'distribution')
 -[f with weight  < 2]-> (w WITH position = 'core')
 -[g with weight  < 2]-> (x WITH position = 'distribution')\end{verbatim}
\textbf{Peak Memory: 2'210'824 kb}\\
\textbf{Result size: 49'956}\\
6 Joins and 7 Selections. This is probably the most interesting result. We add two more joins to the query, but we restrict the number of edges used in all the joins dramatically. This consequently leads to a surprisingly low evaluation time, almost on the level as query 5. This demonstrates how big the impact of the joins is for the evaluation time.\\\\
\textbf{Dataflow}
\begin{figure}[H]
\includegraphics[width=0.8\textwidth]{graph7}
\caption{Dataflow Query7}
\end{figure}
\clearpage
\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.475\textwidth}
        \centering
        \includegraphics[width=\textwidth]{boxesfl/q7}
        \caption[Network2]%
        {{\small Large Fat Tree}}    
        \label{fig:mean and std of net14}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.475\textwidth}  
        \centering 
        \includegraphics[width=\textwidth]{boxesfs/q7}
        \caption[]%
        {{\small Small Fat Tree}}    
        \label{fig:mean and std of net24}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.475\textwidth}   
        \centering 
        \includegraphics[width=\textwidth]{boxesjl/q7}
        \caption[]%
        {{\small Large Jellyfish}}    
        \label{fig:mean and std of net34}
    \end{subfigure}
    \quad
    \begin{subfigure}[b]{0.475\textwidth}   
        \centering 
        \includegraphics[width=\textwidth]{boxesjs/q7}
        \caption[]%
        {{\small Small Jellyfish}}    
        \label{fig:mean and std of net44}
    \end{subfigure}
    \caption[  Boxplots for query 7 ]
    {\small Boxplots for query 7} 
    \label{fig:mean and std of nets}
\end{figure*}
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}
In this chapter we give an overview of other existing graph databases and query evluators.

\subsection{SPARQL}
RDF\footnote{\url{https://www.w3.org/RDF/}} is a directed, labeled graph data format for representing information in the Web. This specification defines the syntax and semantics of the SPARQL query language for RDF. SPARQL can be used to express queries across diverse data sources, whether the data is stored natively as RDF or viewed as RDF via middleware. SPARQL contains capabilities for querying required and optional graph patterns along with their conjunctions and disjunctions. SPARQL also supports extensible value testing and constraining queries by source RDF graph. The results of SPARQL queries can be results sets or RDF graphs.\\\\
The major difference between SPARQL and PGQL is the format of the graph. SPARQL only supports graphs in RDF format, whereas PGQL requires the graph to be in either the edge-list or adjacency-list format.\cite{SPARQL}
\subsection{PQL}
A program query language, PQL for short, is a source language-independent notation to specify program queries and program views. PQL is used as an interface to Static Program Analyzers (SPA), interactive tools that enhance program understanding by answering queries about programs. Queris on global program design as well as searches for detail code patterns are both possible in PQL. Program queries and patterns supported by other notations described in literature and those supported by commercial tools can be written simply and naturally in PQL.\cite {PQL}



\subsection{Green-Marl}
Green-Marl is a domain-specific language (DSL) with high level language construct that allow developers to describe their graph analysis algorithms intuitively, but expose the data-level parallelism inherent in the algorithms. Green-Marl comes with its own compiler which translates high-level algorithmic description written in Green-Marl into an efficient C++ implementation by exploiting this exposed datalevel parallelism. Furthermore, the Green-Marl compiler applies a set of optimizations that take advantage of the high-level semantic knowledge encoded in the Green-Marl DSL. Most graph analysis algorithms can be written very intuitively with Green-Marl and experimental results show that the compiler-generated implementation out of such descriptions performs just as well as or better than highly-tuned handcoded implementations.\cite {Greenmarl}

\subsection{Gremlin}
Developed by the Apache Software Foundation, Gremlin is a query language as well as a graph traversal machine. The graph traversal machine Gremlin consists of three parts that continously interact with each other: first the graph, second the traversal and finally the set of traversers. The traversers move about the graph according to the instructions specified in the traversal, where the result of the computation is the ultimate locations of all halted traversers. A Gremlin machine can be executed over any supporting graph computing system such as an OLTP graph database and/or an OLAP graph processor. The language Gremlin is a functional language implemented in the users native programming language. Gremlin supports both imperative and declarative querying.
\cite{Gremlin}


\subsection{SQLGraph}
SQLGraph is a Graph Store that combines existing relational optimizers with a novel schema, in an attempt to give better performance for property graph storage and retrieval than popular noSQL graph stores. The schema combines relational storage for adjacency information with JSON storage for vertex and edge attributes. This particular schema design has benefits compared to a purely relational or purely JSON solution. The query translation mechanism translates Gremlin queries with no side effects into SQL queries so that one can leverage relational query optimizers. \cite{Sun:2015}


\subsection{GraphiQL}
GRAPHiQL is an intuitive query language for graph analytics, which allows developers to reason in terms of nodes and edges rather than the tables and joins which are used in relational databases. GRAPHiQL provides key graph constructs such as looping, recursion, and neighborhood operations. At runtime, GRAPHiQL compiles graph programs into efficient SQL queries that can run on any relational database. \cite {Graphiql}
\clearpage
\subsection{Overview}
\begin{table}[h]
\centering
\caption{Overview of Framworks}
\label{my-label}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lllllll}
                          & \multicolumn{6}{c}{\textbf{Framework}}                                                                                                                                                                                                                                                                                                         \\[0.25cm] \cline{2-7} 
                         \\[0.1cm] & PGQL                                                     & Gremlin                                                         & GraphiQL                                                        & SPARQL                                                           & Green-Marl & PQL                                                             \\[0.25cm] \hline
\\[0.1cm]\textbf{Language}         & Java                                                     & Java                                                            & Javascript                                                      & Java                                                             & C++        & Java                                                            \\[0.25cm]
\textbf{Operating System} & \begin{tabular}[c]{@{}l@{}}Linux,\\ Mac\end{tabular}     & \begin{tabular}[c]{@{}l@{}}Windows,\\ Linux,\\ Mac\end{tabular} & \begin{tabular}[c]{@{}l@{}}Windows,\\ Linux,\\ Mac\end{tabular} & \begin{tabular}[c]{@{}l@{}}Windows,\\ Linux,\\ Mac\end{tabular}  & Linux      & \begin{tabular}[c]{@{}l@{}}Windows,\\ Linux,\\ Mac\end{tabular} \\[0.25cm]
\textbf{Open Source}      & Parser only                                              & yes                                                             & yes                                                             & \begin{tabular}[c]{@{}l@{}}some imple-\\ mentations\end{tabular} & yes        & yes                                                             \\[0.25cm]
\textbf{Graph Format}     & \begin{tabular}[c]{@{}l@{}}Property\\ Graph\end{tabular} &                                                                 &                                                                 & RDF                                                              &            &                                                                
\end{tabular}}
\end{table}
\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Summary}

\subsection{Conclusion}
In this thesis we presented qlidaf, a program to evaluate PGQL queries in a dataflow environment. We showed how our program parsers queries and graphs, and then evaluates said query on the graph.\\\\
Furthermore we provided an thorough evaluation of the performance of our program to showcase its competitveness.
\subsection{Future Work}
\textbf{Shortest Path} \quad Unfortunately, there is currently no support for evaluating path expressions. This is a rather pressing issue, since paths are of great interests in network routing. In a first step the evaluator should be extended with the functionality to find the shortest path. Subsequently, special properties of the paths can be taken into consideration. These include forbidding the visitation of certain vertices or requiring that a certain type of vertex, e.g. a firewall, has to be included in the path atleast one or more times.\\\\
\textbf{Additional Joins} \quad Currently, Joins are only happening when putting an edge between two vertices. Unfortunately, certain problems stem from this. For example, the query: \begin{verbatim}SELECT * WHERE v.age = u.age\end{verbatim} cannot be evaluate at this time. This query requires us to join the vertex set with itself on the age attribute, which does not pose a problem. The underlying difficulty is to recognize that there is an actual need to join the sets. Therefore in order to fix this issue, we would have to extend the handling of the value constraints, so we can recognize these special requirements. \\\\
\textbf{Query Optimizer} \quad In the current version of the program, all queries are evaluate in the same fashion. We start with the selections of the vertex set, join them with the edge set, and finally apply the projections. Pushing the selections down is usually a good practice, however, under certain conditions this approach is suboptimal. Therefore the implemention of a query optimizer is a must once we get to larger graphs and more complicated queries.\\\\
\textbf{Changes in the Graph} \quad Throughout all the experiments, our graphs never changed at all. Since this program was built on differential dataflow, which excels at dealing with dynamic graphs, it would make a lot of sense to have periodic input, that adds or removes parts of the graph.
\subsection{Acknowledgements}
I would like to thank my supervisors John Liagouris, Desislava Dimitrova and Moritz Hoffmann for their continuous support and help. They provided many meaningful suggestions and proposals. Their expertise and experience was invaluable.\\\\
I would also like to thank Professor Roscoe for taking the time to supervise my thesis.

\clearpage


\bibliography{MA}{}
\bibliographystyle{plain}
\end{document}
